{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD27tNLFpn4Z"
      },
      "source": [
        "# Double Deep Q-Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4qvAoHapn4c"
      },
      "source": [
        "Referred Paper: https://arxiv.org/pdf/1509.06461.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIpc_E36pn4d"
      },
      "source": [
        "**Environment:** CartPole-v1\n",
        "\n",
        "**Actions:** 0 to push the cart to the left \n",
        "         1 to push the cart to the right.  \n",
        "\n",
        "To \"solve\" this puzzle we have to have an average reward of > 195 over 100 consecutive episodes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x42mfnB4pn4e",
        "outputId": "bd47bdb3-6808-406c-878d-4ee31469265d"
      },
      "outputs": [],
      "source": [
        "#Imports and gym creation\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import random\n",
        "\n",
        "#Create Gym\n",
        "from gym import wrappers\n",
        "envCartPole = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSup-1Skpn4f"
      },
      "outputs": [],
      "source": [
        "EPISODES = 500\n",
        "TRAIN_END = 0\n",
        "\n",
        "def discount_rate(): #Gamma\n",
        "    return 0.95\n",
        "\n",
        "def learning_rate(): #Alpha\n",
        "    return 0.001\n",
        "\n",
        "def batch_size():\n",
        "    return 24\n",
        "\n",
        "nS = envCartPole.observation_space.shape[0] #This is only 4\n",
        "nA = envCartPole.action_space.n #Actions\n",
        "\n",
        "batch_size = batch_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT0jBSuBpn4g"
      },
      "outputs": [],
      "source": [
        "class DoubleDeepQNetwork():\n",
        "    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay):\n",
        "        self.nS = states\n",
        "        self.nA = actions\n",
        "        self.memory = deque([], maxlen=2500)\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        #Explore/Exploit\n",
        "        self.epsilon = epsilon\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.model = self.build_model()\n",
        "        self.model_target = self.build_model() #Second (target) neural network\n",
        "        self.update_target_from_model() #Update weights\n",
        "        self.loss = []\n",
        "\n",
        "    def build_model(self):\n",
        "        model = keras.Sequential()\n",
        "        model.add(keras.layers.Dense(24, input_dim=self.nS, activation='relu'))\n",
        "        model.add(keras.layers.Dense(24, activation='relu'))\n",
        "        model.add(keras.layers.Dense(self.nA, activation='linear'))\n",
        "        model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=self.alpha))\n",
        "        return model\n",
        "\n",
        "    def update_target_from_model(self):\n",
        "        self.model_target.set_weights( self.model.get_weights() )\n",
        "\n",
        "    def action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.nA) #Explore\n",
        "        action_vals = self.model.predict(state) #Exploit\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def test_action(self, state): #Exploit\n",
        "        action_vals = self.model.predict(state)\n",
        "        return np.argmax(action_vals[0])\n",
        "\n",
        "    def store(self, state, action, reward, nstate, done):\n",
        "        #Store the experience in memory\n",
        "        self.memory.append( (state, action, reward, nstate, done) )\n",
        "\n",
        "    def experience_replay(self, batch_size):\n",
        "        #Execute the experience replay\n",
        "        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n",
        "\n",
        "        x = []\n",
        "        y = []\n",
        "        np_array = np.array(minibatch)\n",
        "        st = np.zeros((0,self.nS)) #States\n",
        "        nst = np.zeros( (0,self.nS) )#Next States\n",
        "        for i in range(len(np_array)): #Creating the state and next state np arrays\n",
        "            st = np.append( st, np_array[i,0], axis=0)\n",
        "            nst = np.append( nst, np_array[i,3], axis=0)\n",
        "        st_predict = self.model.predict(st)\n",
        "        nst_predict = self.model.predict(nst)\n",
        "        nst_predict_target = self.model_target.predict(nst)\n",
        "        index = 0\n",
        "        for state, action, reward, nstate, done in minibatch:\n",
        "            x.append(state)\n",
        "            nst_action_predict_target = nst_predict_target[index]\n",
        "            nst_action_predict_model = nst_predict[index]\n",
        "            if done == True: #Terminal state\n",
        "                target = reward\n",
        "            else:   #Non terminal\n",
        "                target = reward + self.gamma * nst_action_predict_target[np.argmax(nst_action_predict_model)] #Using Q to get T is Double DQN\n",
        "            target_f = st_predict[index]\n",
        "            target_f[action] = target\n",
        "            y.append(target_f)\n",
        "            index += 1\n",
        "\n",
        "        x_reshape = np.array(x).reshape(batch_size,self.nS)\n",
        "        y_reshape = np.array(y)\n",
        "        epoch_count = 1\n",
        "        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n",
        "        for i in range(epoch_count):\n",
        "            self.loss.append( hist.history['loss'][i] )\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeTy-wewpn4h",
        "outputId": "b3bc4335-3415-44a6-ab4d-f9e680ca9ac3"
      },
      "outputs": [],
      "source": [
        "dqn = DoubleDeepQNetwork(nS, nA, learning_rate(), discount_rate(), 1, 0.001, 0.995 )\n",
        "\n",
        "#Training\n",
        "rewards = [] #Store rewards for graphing\n",
        "epsilons = [] # Store the Explore/Exploit\n",
        "TEST_Episodes = 0\n",
        "for e in range(EPISODES):\n",
        "    state = envCartPole.reset()\n",
        "    state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n",
        "    tot_rewards = 0\n",
        "    for time in range(200):\n",
        "        action = dqn.action(state)\n",
        "        nstate, reward, done, _ = envCartPole.step(action)\n",
        "        nstate = np.reshape(nstate, [1, nS])\n",
        "        tot_rewards += reward\n",
        "        dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n",
        "        state = nstate\n",
        "        #done: CartPole fell.\n",
        "        #time == 199: CartPole stayed upright\n",
        "        if done or time == 199:\n",
        "            rewards.append(tot_rewards)\n",
        "            epsilons.append(dqn.epsilon)\n",
        "            print(\"episode: {}/{}, \\t score: {}, \\t e: {}\"\n",
        "                  .format(e, EPISODES, tot_rewards, dqn.epsilon))\n",
        "            break\n",
        "        #Experience Replay\n",
        "        if len(dqn.memory) > batch_size:\n",
        "            dqn.experience_replay(batch_size)\n",
        "    #Update the weights after each episode (You can configure this for x steps as well\n",
        "    dqn.update_target_from_model()\n",
        "    #If our current NN passes we are done\n",
        "    #I am going to use the last 5 runs\n",
        "    # if len(rewards) > 5 and np.average(rewards[-5:]) > 195:\n",
        "    #     #Set the rest of the EPISODES for testing\n",
        "    #     TEST_Episodes = EPISODES - e\n",
        "    #     TRAIN_END = e\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiiUs78Hpn4i",
        "outputId": "8f19e9b2-0a3c-45cf-a59c-1af98bece022"
      },
      "outputs": [],
      "source": [
        "# #Testing\n",
        "# print('Training complete. Testing started...')\n",
        "# #TEST Time\n",
        "# #   In this section we ALWAYS use exploit don't train any more\n",
        "# for e_test in range(TEST_Episodes):\n",
        "#     state = envCartPole.reset()\n",
        "#     state = np.reshape(state, [1, nS])\n",
        "#     tot_rewards = 0\n",
        "#     for t_test in range(210):\n",
        "#         action = dqn.test_action(state)\n",
        "#         nstate, reward, done, _ = envCartPole.step(action)\n",
        "#         nstate = np.reshape( nstate, [1, nS])\n",
        "#         tot_rewards += reward\n",
        "#         #DON'T STORE ANYTHING DURING TESTING\n",
        "#         state = nstate\n",
        "#         #done: CartPole fell.\n",
        "#         #t_test == 209: CartPole stayed upright\n",
        "#         if done or t_test == 209:\n",
        "#             rewards.append(tot_rewards)\n",
        "#             epsilons.append(0) #We are doing full exploit\n",
        "#             print(\"episode: {}/{}, \\t score: {}, \\t e: {}\"\n",
        "#                   .format(e_test, TEST_Episodes, tot_rewards, 0))\n",
        "#             break;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMFLKUznpn4j",
        "outputId": "3a95ed7e-d6a4-459c-f709-410803afb435"
      },
      "outputs": [],
      "source": [
        "rolling_average = np.convolve(rewards, np.ones(100)/100)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(rewards, label='Rewards')\n",
        "plt.plot(rolling_average, color='black', label='Average Scores')\n",
        "eps_graph = [200*x for x in epsilons]\n",
        "plt.plot(eps_graph, color='g', linestyle='-', label='Epsilon (0.001 - 1.0)')\n",
        "plt.xlim((0, EPISODES))\n",
        "plt.ylim((0, 220))\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
